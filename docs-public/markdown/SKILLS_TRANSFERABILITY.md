# ğŸ”„ Skills Transferability: Learning Lab â†’ Production Systems

## Overview

Your production trading exchange uses **C++, Python, and Java**. This Go-based learning lab teaches **SRE techniques** that are **100% language-agnostic** and apply directly to your real systems - even without modifying application code.

**Key Point:** You're not learning Go. You're learning **observability, chaos engineering, and performance analysis** - skills that work on any language.

## Direct Transferability Map

### âœ… 100% Transferable (No Code Changes Needed)

These skills transfer **exactly** to C++/Python/Java production systems:

| Skill Learned | How It Transfers | Production Application |
|---------------|------------------|------------------------|
| **eBPF Tools** | Language-agnostic kernel tracing | Trace C++/Python/Java apps identically |
| **Chaos Engineering** | Infrastructure-level, not code-level | Kill containers, throttle CPU, partition network |
| **Flamegraph Analysis** | Works with native profiles from any language | `perf` for C++, `py-spy` for Python, `async-profiler` for Java |
| **Prometheus Metrics** | HTTP endpoint scraping | Apps just expose `/metrics` endpoint |
| **Grafana Dashboards** | Visualization of any metric source | Same dashboards, different data sources |
| **Load Testing** | External HTTP/API testing | Test any API regardless of language |
| **Incident Response** | Process and methodology | Same runbooks apply |
| **Log Analysis** | Structured logging patterns | Parse JSON logs from any language |
| **Container Orchestration** | Docker/K8s platform skills | Same commands, different images |
| **Performance Profiling** | Methodology and interpretation | Flamegraphs look similar across languages |

### ğŸ”¶ Partially Transferable (Adapt Tools/Approach)

These skills require tool changes but the **methodology** is identical:

| Skill Learned | Tool in Lab | Tool in Production | Concept Transferability |
|---------------|-------------|-------------------|------------------------|
| **CPU Profiling** | `pprof` (Go) | `perf` (C++), `cProfile` (Python), `async-profiler` (Java) | 100% - flamegraphs all work the same way |
| **Memory Profiling** | `pprof --heap` | `valgrind` (C++), `memory_profiler` (Python), `JFR` (Java) | 100% - find leaks, allocations |
| **Application Metrics** | Prometheus Go client | Prometheus C++/Python/Java clients | 100% - same metric types (counter, gauge, histogram) |
| **Tracing** | Go runtime | `strace` (C++), `strace` (Python), `jstack` (Java) | 100% - system call analysis |

### âŒ Not Transferable (Language-Specific)

These are Go-specific and don't apply:

| Go-Specific Skill | Why Not Transferable | What IS Transferable |
|-------------------|---------------------|----------------------|
| Goroutine profiling | C++/Java use threads, Python has GIL | **Concurrency analysis methodology** - understand parallelism bottlenecks |
| Go runtime tuning | Different runtime per language | **Performance tuning methodology** - measure, hypothesis, test, validate |
| Go code patterns | Different language syntax | **Microservices architecture patterns** - API design, service boundaries |

## The 90/10 Rule

**90% of SRE work** is **infrastructure and observability** - completely language-agnostic.

**10% of SRE work** is **application code changes** - you said you can't do this anyway!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SRE Skills You're Learning (100%)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  90% - Infrastructure & Observability (âœ… Transfers) â”‚
â”‚  â”œâ”€ eBPF tracing                                    â”‚
â”‚  â”œâ”€ Chaos engineering                               â”‚
â”‚  â”œâ”€ Metrics & dashboards                            â”‚
â”‚  â”œâ”€ Performance profiling                           â”‚
â”‚  â”œâ”€ Incident response                               â”‚
â”‚  â”œâ”€ Load testing                                    â”‚
â”‚  â””â”€ Log analysis                                    â”‚
â”‚                                                      â”‚
â”‚  10% - Code-Level Changes (âŒ Can't do anyway)       â”‚
â”‚  â””â”€ Modifying application code                      â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Detailed Transferability by SRE Domain

### 1. Chaos Engineering (100% Transferable)

**What you learn in lab:**
```bash
# Kill a service
docker kill simulated-exchange-trading-api

# Throttle CPU
docker update --cpus="0.5" simulated-exchange-trading-api

# Network partition
iptables -A INPUT -p tcp --dport 8080 -j DROP
```

**How it applies to production C++/Python/Java:**
```bash
# Exact same commands!
docker kill production-cpp-trading-engine
docker update --cpus="0.5" production-python-risk-manager
iptables -A INPUT -p tcp --dport 8080 -j DROP
```

**Why it transfers:** Chaos engineering targets **infrastructure**, not code. Language doesn't matter.

**Learning outcomes:**
- âœ… Understanding failure modes (same in any language)
- âœ… Designing resilient systems (language-agnostic patterns)
- âœ… Testing recovery mechanisms (container/OS level)
- âœ… Validating monitoring alerts (metrics are metrics)

### 2. eBPF & System Tracing (100% Transferable)

**What you learn in lab:**
```bash
# Trace system calls
sudo bpftrace -e 'tracepoint:syscalls:sys_enter_* { @[probe] = count(); }'

# Network latency
sudo tcplife

# Block I/O latency
sudo biolatency

# CPU sampling
sudo profile-bpfcc -p <PID> 30
```

**How it applies to production:**
```bash
# EXACTLY THE SAME - eBPF works at kernel level!
sudo bpftrace -e 'tracepoint:syscalls:sys_enter_* { @[probe] = count(); }' -p <cpp_app_pid>
sudo tcplife -p <python_app_pid>
sudo biolatency
sudo profile-bpfcc -p <java_app_pid> 30
```

**Why it transfers:** eBPF traces the **Linux kernel**, not the application language.

**Production example - C++ trading engine:**
```bash
# Find what system calls your C++ app makes
sudo bpftrace -p $(pgrep trading-engine) -e '
  tracepoint:syscalls:sys_enter_* {
    @syscalls[probe] = count();
  }
'

# Trace network latency for Java risk calculator
sudo tcplife -p $(pgrep java.*risk)

# Profile C++ order matcher CPU usage
sudo perf record -p $(pgrep order-matcher) -g -- sleep 30
sudo perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl > cpp_flamegraph.svg
```

**Learning outcomes:**
- âœ… Using eBPF tools safely (overhead, safety, interpretation)
- âœ… System-level performance analysis (works on any process)
- âœ… Network and I/O profiling (kernel-level visibility)
- âœ… Understanding system behavior (how apps interact with OS)

### 3. Performance Profiling & Flamegraphs (95% Transferable)

**What you learn in lab:**
```bash
# Generate CPU flamegraph
./scripts/generate-flamegraph.sh cpu 30 8080
# Creates: flamegraph.svg + AI analysis
```

**How it applies to production:**

#### C++ Application:
```bash
# Use perf (built into Linux)
sudo perf record -F 99 -p $(pgrep cpp-trading-app) -g -- sleep 30
sudo perf script | stackcollapse-perf.pl | flamegraph.pl > cpp-trading.svg

# View in browser - looks just like Go flamegraph!
firefox cpp-trading.svg
```

#### Python Application:
```bash
# Use py-spy (works on running Python without code changes)
sudo py-spy record -p $(pgrep python.*risk) -o python-risk.svg -d 30

# View in browser - same flamegraph format!
firefox python-risk.svg
```

#### Java Application:
```bash
# Use async-profiler (works on running JVM)
./profiler.sh -d 30 -f java-settlement.svg $(pgrep java.*settlement)

# View in browser - same flamegraph format!
firefox java-settlement.svg
```

**Why it transfers:** Flamegraphs are a **visualization technique** for stack traces. Same mental model, different tools.

**Learning outcomes:**
- âœ… Reading flamegraphs (identical visual interpretation)
- âœ… Identifying hotspots (wide bars = slow code)
- âœ… Understanding call stacks (parent-child relationships)
- âœ… Profiling methodology (baseline â†’ load â†’ compare)
- âœ… AI analysis patterns (similar optimization recommendations)

### 4. Metrics & Dashboards (100% Transferable)

**What you learn in lab:**
```yaml
# Prometheus scrapes /metrics endpoint
scrape_configs:
  - job_name: 'trading-api'
    static_configs:
      - targets: ['trading-api:8080']
```

**How it applies to production:**
```yaml
# Exact same pattern for C++/Python/Java
scrape_configs:
  - job_name: 'cpp-trading-engine'
    static_configs:
      - targets: ['cpp-engine:9090']  # C++ app with Prometheus exporter

  - job_name: 'python-risk-manager'
    static_configs:
      - targets: ['risk-mgr:8000']    # Python app with prometheus_client

  - job_name: 'java-settlement'
    static_configs:
      - targets: ['settlement:8080']   # Java app with Micrometer
```

**Application-side (one-time code change, then you're done):**

C++ (using prometheus-cpp):
```cpp
// Expose /metrics endpoint - done once, never touch again
#include <prometheus/exposer.h>
prometheus::Exposer exposer{"0.0.0.0:9090"};
// Rest is automatic
```

Python (using prometheus_client):
```python
# Expose /metrics endpoint - done once, never touch again
from prometheus_client import start_http_server
start_http_server(8000)
# Rest is automatic
```

Java (using Micrometer):
```java
// Expose /metrics endpoint - done once, never touch again
@Bean
MeterRegistry meterRegistry() {
    return new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);
}
// Rest is automatic
```

**After that, all your SRE work is identical:**
- âœ… Grafana dashboards (same queries, different targets)
- âœ… Alerting rules (same Prometheus rules)
- âœ… Metric analysis (same RED/USE methodology)

**Learning outcomes:**
- âœ… Grafana dashboard design (100% transferable)
- âœ… PromQL queries (100% transferable)
- âœ… Alerting strategies (100% transferable)
- âœ… Metric patterns (RED/USE/Golden Signals - language-agnostic)

### 5. Load Testing (100% Transferable)

**What you learn in lab:**
```bash
# HTTP load testing
ab -n 10000 -c 100 http://localhost:8080/api/orders

# Custom scenarios
k6 run loadtest.js
```

**How it applies to production:**
```bash
# Exactly the same - you're testing the API, not the code!
ab -n 10000 -c 100 https://prod-cpp-trading.com/api/orders
k6 run production-loadtest.js
```

**Why it transfers:** Load testing is **black box** - you hit APIs, measure responses. Language is irrelevant.

**Learning outcomes:**
- âœ… Load test design (ramp-up, steady-state, spike)
- âœ… Interpreting results (latency, throughput, errors)
- âœ… Capacity planning (understanding limits)
- âœ… Bottleneck identification (where does it break?)

### 6. Incident Response (100% Transferable)

**What you learn in lab:**
```bash
# Incident workflow
1. Detect (alerts fire)
2. Triage (check dashboards)
3. Investigate (logs, metrics, traces)
4. Mitigate (restart service, scale up)
5. Resolve (fix root cause)
6. Document (postmortem)
```

**How it applies to production:**
```bash
# EXACTLY THE SAME PROCESS
1. Detect (PagerDuty alert for C++ engine)
2. Triage (check Grafana for Python risk service)
3. Investigate (query logs from Java settlement)
4. Mitigate (restart containers, scale deployment)
5. Resolve (coordinate with dev team)
6. Document (write postmortem)
```

**Learning outcomes:**
- âœ… Incident response methodology (completely language-agnostic)
- âœ… Using runbooks (process, not code)
- âœ… Debugging techniques (logs, metrics, traces work everywhere)
- âœ… Communication patterns (stakeholder updates)

### 7. Container & Orchestration (100% Transferable)

**What you learn in lab:**
```bash
# Docker operations
docker ps
docker logs -f simulated-exchange-trading-api
docker restart simulated-exchange-trading-api
docker stats

# Docker Compose
docker-compose up -d
docker-compose logs -f trading-api
docker-compose restart trading-api
```

**How it applies to production:**
```bash
# IDENTICAL - containers don't care about language!
docker ps
docker logs -f production-cpp-trading-engine
docker restart production-python-risk-manager
docker stats

docker-compose up -d
docker-compose logs -f java-settlement-service
docker-compose restart cpp-order-matcher
```

**Kubernetes (if you use it):**
```bash
# Same skills, same commands
kubectl get pods
kubectl logs -f cpp-trading-engine-7d9f8b6c4-xyz
kubectl describe pod python-risk-manager-abc123
kubectl exec -it java-settlement-pod -- bash
```

**Learning outcomes:**
- âœ… Container lifecycle management (language-independent)
- âœ… Log aggregation (works for any containerized app)
- âœ… Resource management (CPU/memory limits apply to all)
- âœ… Orchestration concepts (same in K8s/Docker Swarm)

## Real-World Production Examples

### Example 1: High Latency in C++ Trading Engine

**Skills from learning lab:**
1. **Grafana dashboards** - Detect latency spike
2. **eBPF tracing** - Identify system call bottleneck
3. **Flamegraph profiling** - Find hot code path
4. **Incident response** - Follow runbook

**How you'd apply it:**
```bash
# 1. Detect in Grafana (same dashboard skills)
# Alert: "C++ Trading Engine P99 latency > 100ms"

# 2. Check metrics (same PromQL skills)
rate(cpp_order_processing_duration_seconds_sum[5m]) /
rate(cpp_order_processing_duration_seconds_count[5m])

# 3. Trace with eBPF (exact same tool)
sudo bpftrace -p $(pgrep trading-engine) -e '
  tracepoint:syscalls:sys_enter_futex {
    @futex_calls = count();
  }
'
# Find: excessive futex calls = lock contention

# 4. Profile with perf (similar to Go pprof)
sudo perf record -p $(pgrep trading-engine) -g -- sleep 30
sudo perf script | stackcollapse-perf.pl | flamegraph.pl > engine.svg
# Flamegraph shows: 40% time in std::mutex::lock()

# 5. Share findings with C++ developers
# "Lock contention in OrderBook::matchOrder() - see flamegraph"
```

**Result:** You didn't change C++ code, but you identified the exact problem for developers to fix.

### Example 2: Memory Leak in Python Risk Calculator

**Skills from learning lab:**
1. **Memory profiling methodology**
2. **Metrics and alerting**
3. **Container resource monitoring**

**How you'd apply it:**
```bash
# 1. Detect in Grafana (same skills)
# Alert: "Python Risk Service memory usage > 8GB"

# 2. Profile with py-spy (like pprof for Go)
sudo py-spy record --format speedscope -p $(pgrep python.*risk) -o risk.json

# 3. Analyze with eBPF memory profiling
sudo bpftrace -e '
  tracepoint:kmem:kmalloc {
    @bytes = hist(args->bytes_alloc);
  }
' -p $(pgrep python)

# 4. Check Python-specific memory
import tracemalloc  # Could add this to app
# Or use guppy/heapy for running process

# 5. Docker memory monitoring (same as Go)
docker stats python-risk-calculator
```

**Result:** Identified memory leak pattern, gave evidence to Python developers.

### Example 3: Network Issues in Java Settlement Service

**Skills from learning lab:**
1. **Network tracing with eBPF**
2. **Chaos engineering (network partition)**
3. **Distributed tracing concepts**

**How you'd apply it:**
```bash
# 1. Detect timeouts (Grafana alerts - same skills)
# Alert: "Java Settlement Service timeout rate > 5%"

# 2. Trace network with eBPF (exact same tool)
sudo tcplife -p $(pgrep java.*settlement)
# Shows: connections to database taking 500ms

# 3. Test with chaos (same chaos scripts)
# Inject 100ms latency to database
tc qdisc add dev eth0 root netem delay 100ms

# 4. Profile Java service (similar to Go)
./async-profiler.sh -d 30 -o flamegraph settlement.svg $(pgrep java)

# 5. Check JVM metrics (like Go runtime metrics)
jstat -gcutil $(pgrep java) 1000
# Shows: frequent GC pauses
```

**Result:** Found network + GC issues without touching Java code.

## Transferability by Learning Path

### Week 1-2: Observability (100% Transferable)

| Skill | Lab Tool | Production Tool | Transferability |
|-------|----------|-----------------|-----------------|
| Metrics collection | Prometheus | Prometheus | 100% - same queries |
| Dashboards | Grafana | Grafana | 100% - same UI/queries |
| Log analysis | Docker logs | Docker/K8s logs | 100% - same format |
| Tracing | (Concepts) | Jaeger/Zipkin | 100% - same principles |

**What transfers:** All of it. Observability is infrastructure-level.

### Week 3-4: Chaos Engineering (100% Transferable)

| Skill | Lab Chaos | Production Chaos | Transferability |
|-------|-----------|------------------|-----------------|
| Service failure | Kill Go container | Kill C++/Python/Java container | 100% |
| CPU throttling | Limit Go CPU | Limit any container CPU | 100% |
| Network partition | iptables rules | Same iptables rules | 100% |
| Disk I/O | I/O limits | Same I/O limits | 100% |

**What transfers:** Everything. Chaos engineering targets infrastructure, not code.

### Week 5-6: eBPF & Profiling (95% Transferable)

| Skill | Lab Tool | Production Tool | Transferability |
|-------|----------|-----------------|-----------------|
| System call tracing | bpftrace | bpftrace | 100% - kernel level |
| Network tracing | tcplife | tcplife | 100% - kernel level |
| CPU profiling | pprof | perf/py-spy/async-profiler | 95% - different tool, same concepts |
| Memory profiling | pprof --heap | valgrind/memory_profiler/JFR | 90% - different tool, same concepts |

**What transfers:** Methodology 100%, tools 95% (different profilers, same interpretation).

### Week 7-8: Performance Optimization (80% Transferable)

| Skill | Transferability | Why |
|-------|----------------|-----|
| Identifying bottlenecks | 100% | Flamegraphs show hotspots in any language |
| Measuring improvements | 100% | Metrics and benchmarks work the same |
| Applying optimizations | 0% | Can't change production code |
| Validating fixes | 100% | Load testing and profiling apply equally |

**What transfers:** Analysis and validation. You identify problems and measure results - developers fix code.

## Skills That Make You Effective Even Without Code Access

### 1. **Problem Identification Expert**

**What you learn:**
- Reading flamegraphs to find hotspots
- Using eBPF to identify system bottlenecks
- Metrics analysis to detect anomalies

**How it helps:**
- You become the "performance detective"
- Give developers **precise evidence**: "Lock contention in OrderBook.cpp line 423"
- Reduce debugging time from days to hours

### 2. **Infrastructure Resilience Engineer**

**What you learn:**
- Chaos engineering patterns
- Failure mode analysis
- Recovery automation

**How it helps:**
- Design resilient deployment strategies
- Configure proper resource limits
- Set up auto-recovery mechanisms
- All **without touching application code**

### 3. **Observability Architect**

**What you learn:**
- Designing effective dashboards
- Creating meaningful alerts
- Implementing SLI/SLO/SLA

**How it helps:**
- Build monitoring that works for C++/Python/Java
- Detect problems before customers do
- Provide data-driven capacity planning

## Concrete Production Workflow Example

**Scenario:** P99 latency increased in C++ trading engine from 20ms to 200ms.

### What You Do (Using Learning Lab Skills):

```bash
# 1. Detect (Grafana dashboard - learned in lab)
#    Alert fires: "Trading Engine Latency SLO Breach"

# 2. Triage (Prometheus queries - learned in lab)
histogram_quantile(0.99,
  rate(order_processing_duration_bucket[5m])
)
# Confirms: P99 latency is 200ms

# 3. System-level investigation (eBPF - learned in lab)
sudo bpftrace -p $(pgrep trading-engine) -e '
  tracepoint:sched:sched_switch {
    @[kstack] = count();
  }
'
# Finds: excessive context switching

# 4. CPU profiling (perf - similar to pprof learned in lab)
sudo perf record -F 99 -p $(pgrep trading-engine) -g -- sleep 30
sudo perf script | stackcollapse-perf.pl | flamegraph.pl > engine.svg
# Flamegraph shows: 60% time in std::vector::push_back()

# 5. Memory investigation (eBPF - learned in lab)
sudo bpftrace -e '
  tracepoint:kmem:kmalloc {
    @alloc_bytes[comm] = sum(args->bytes_alloc);
  }
'
# Finds: excessive allocations (2GB/sec)

# 6. Evidence package for developers
cat > incident-report.md << EOF
## Incident: Trading Engine Latency Spike

**When:** 2025-11-16 14:30 UTC
**Impact:** P99 latency 20ms â†’ 200ms (10x increase)

**Root Cause Analysis:**
1. CPU profiling shows 60% time in vector::push_back()
   - See flamegraph: engine.svg
   - Code location: OrderBook.cpp line 423

2. Memory profiling shows 2GB/sec allocation rate
   - Likely repeated vector resizing
   - eBPF trace shows 50K allocations/sec

3. Context switching increased 10x
   - Likely due to memory pressure causing page faults

**Recommendation for C++ Team:**
- Pre-allocate vector capacity in OrderBook::addOrder()
- Consider object pooling to reduce allocations
- Current: vector.push_back() â†’ resize every time
- Suggested: vector.reserve(expected_size)

**Evidence:**
- Flamegraph: [engine.svg](engine.svg)
- Metrics: [grafana dashboard](link)
- eBPF traces: [bpftrace output](trace.txt)
EOF

# 7. Validation after fix (load testing - learned in lab)
ab -n 100000 -c 100 https://prod-trading/api/orders
# Verify: P99 latency back to 20ms
```

**Result:** You didn't write a line of C++, but you:
- Detected the problem (Grafana)
- Identified root cause (eBPF + profiling)
- Provided precise evidence (flamegraphs)
- Validated the fix (load testing)

**All skills learned in the Go lab applied directly to C++ production.**

## Summary: What Actually Matters

### âœ… These Skills Transfer 100%:

1. **Observability**
   - Metrics, dashboards, alerts
   - Log analysis
   - Distributed tracing concepts

2. **Chaos Engineering**
   - Failure injection
   - Resilience testing
   - Recovery validation

3. **eBPF & System Tracing**
   - System call analysis
   - Network tracing
   - I/O profiling

4. **Performance Profiling**
   - Flamegraph interpretation
   - Hotspot identification
   - Bottleneck analysis

5. **Incident Response**
   - Runbooks and processes
   - Debugging methodology
   - Root cause analysis

### ğŸ”§ These Tools Change, Concepts Don't:

| Go Lab Tool | C++ Production | Python Production | Java Production |
|-------------|----------------|-------------------|-----------------|
| `pprof` | `perf` | `py-spy` | `async-profiler` |
| Go runtime metrics | C++ app metrics | Python metrics | JVM metrics |
| Go traces | `strace` | `strace` | `jstack` |

**But:** The way you **use** and **interpret** them is identical.

### âŒ These Don't Transfer (And You Can't Do Them Anyway):

1. **Code modifications** - You said you can't change production code
2. **Language-specific optimizations** - Requires code changes
3. **Runtime tuning** - Different per language (but methodology transfers)

## The Bottom Line

**90% of what you learn in this lab applies directly to C++/Python/Java production systems.**

The 10% that doesn't transfer is **code-level changes you can't make anyway**.

**You're learning:**
- **How to find problems** (observability)
- **How to prove resilience** (chaos engineering)
- **How to measure performance** (profiling)
- **How to respond to incidents** (runbooks)

**You're NOT learning:**
- How to write Go code
- Go-specific optimizations
- Go runtime internals

**This makes you a highly effective SRE who can:**
- Identify problems precisely
- Provide evidence to developers
- Validate fixes
- Design resilient systems
- Respond to incidents

All **without touching application code** - which is exactly your constraint.

---

**Next:** [Start onboarding](SRE_ONBOARDING.md) and begin learning these transferable skills!
